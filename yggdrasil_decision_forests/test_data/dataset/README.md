# Datasets for unit testing

## Adult

Full name: Adult (also known as Census)

Url: https://archive.ics.uci.edu/ml/datasets/adult

Donors: Ronny Kohavi and Barry Becker

## DNA

Full name: Molecular Biology (Splice-junction Gene Sequences) Data Set

Url:
https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+(Splice-junction+Gene+Sequences)

Donors: G. Towell, M. Noordewier, and J. Shavlik

## Iris

Full name: Iris

Url: https://archive.ics.uci.edu/ml/datasets/iris

Creator: R.A. Fisher

Donors: Michael Marshall

## SST Binary

Full name: Stanford Sentiment Treebank; Binary classification

Url: https://nlp.stanford.edu/sentiment/index.html

Authors: Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher
Manning, Andrew Ng and Christopher Potts

## Toy

Full name: Yggdrasil Toy

Url: *This location*

Author: Mathieu Guillame-Bert

CSV and TFRecord showing how Yggdrasil Decision Forests represents various types
of features. The dataset contains 5 examples with different types of features (
numerical, categorical as integer, categorical as string, categorical set, and
boolean). Contains some missing values.

## Yggdrasil Synthetic Ranking

Full name: Yggdrasil Synthetic Ranking

Url: *This location*

Author: Mathieu Guillame-Bert

Small ranking dataset containing 500 groups with 10 items each. The
label/relevance is in [0,5]. Contains numerical and categorical features.
Contains missing values. Does NOT contains categorical-set and multi-dimensional
features. The NDCG@5 of random predictions is ~0.50. A basic model without
tuning can reach an NDCG@5 of ~0.77.

Generated with the Yggdrasil Synthetic Dataset Generator with the command:

```
echo "ranking{} num_categorical_set: 0" > /tmp/synthetic_ranking_config.pbtxt
bazel run -c opt --copt=-mavx2 //third_party/yggdrasil_decision_forests/cli/utils:synthetic_dataset -- \
    --alsologtostderr \
    --options=/tmp/synthetic_ranking_config.pbtxt\
    --train=csv:/tmp/synthetic_ranking_train.csv \
    --test=csv:/tmp/synthetic_ranking_test.csv \
    --ratio_test=0.2
```

## Sim PTE

Full name: Simulations for Personalized Treatment Effects

Generated with the R's Uplift package:
https://cran.r-project.org/web/packages/uplift/uplift.pdf

Creator: Leo Guelman <leo.guelman@gmail.com>

Code:

```r
library(uplift)

set.seed(123)

train <- sim_pte(n = 1000, p = 20, rho = 0, sigma = sqrt(2), beta.den = 4)
test <- sim_pte(n = 2000, p = 20, rho = 0, sigma = sqrt(2), beta.den = 4)

train$treat <- ifelse(train$treat == 1, 2, 1)
test$treat <- ifelse(test$treat == 1, 2, 1)

train$y <- ifelse(train$y == 1, 2, 1)
test$y <- ifelse(test$y == 1, 2, 1)

train$ts = NULL
test$ts = NULL

write.csv(train,"yggdrasil_decision_forests/test_data/dataset/sim_pte_train.csv", row.names=F, quote=F)
write.csv(test,"yggdrasil_decision_forests/test_data/dataset/sim_pte_test.csv", row.names=F, quote=F)
```

## Gaussians

Generate two gaussians for anomaly detection similarly as:
https://scikit-learn.org/stable/auto_examples/ensemble/plot_isolation_forest.html

```python
def gen_ds(n_samples: int = 120, n_outliers: int = 40, seed: int = 0):
    np.random.seed(seed)
    covariance = np.array([[0.5, -0.1], [0.7, 0.4]])
    cluster_1 = 0.4 * np.random.randn(n_samples, 2) @ covariance + np.array(
        [2, 2]
    )
    cluster_2 = 0.3 * np.random.randn(n_samples, 2) + np.array([-2, -2])
    outliers = np.random.uniform(low=-4, high=4, size=(n_outliers, 2))
    features = np.concatenate([cluster_1, cluster_2, outliers])
    labels = np.concatenate([
        np.zeros((2 * n_samples), dtype=bool),
        np.ones((n_outliers), dtype=bool),
    ])
    return features, labels
```

## toy_codex-null.avro & toy_codex-deflate.avro

Avro files generated by the following script:

```python
import math
from fastavro import parse_schema, reader, writer

schema = {
    "name": "ToyDataset",
    "doc": "A toy dataset.",
    "type": "record",
    "fields": [
        {"name": "f_null", "type": "null"},
        {"name": "f_boolean", "type": "boolean"},
        {"name": "f_int", "type": "int"},
        {"name": "f_long", "type": "long"},
        {"name": "f_float", "type": "float"},
        {"name": "f_another_float", "type": "float"},
        {"name": "f_double", "type": "double"},
        {"name": "f_string", "type": "string"},
        {"name": "f_bytes", "type": "bytes"},
        {"name": "f_float_optional", "type": ["null", "float"]},
        {
            "name": "f_array_of_float",
            "type": {"type": "array", "items": "float"},
        },
        {
            "name": "f_array_of_double",
            "type": {"type": "array", "items": "double"},
        },
        {
            "name": "f_array_of_string",
            "type": {"type": "array", "items": "string"},
        },
        {
            "name": "f_another_array_of_string",
            "type": {"type": "array", "items": "string"},
        },
        {
            "name": "f_optional_array_of_float",
            "type": ["null", {"type": "array", "items": "float"}],
        },
    ],
}
parsed_schema = parse_schema(schema)
print(parsed_schema)

records = [
    {
        "f_null": None,
        "f_boolean": True,
        "f_int": 5,
        "f_long": 1234567,
        "f_float": 3.1415,
        "f_another_float": 5.0,
        "f_double": math.nan,
        "f_string": "hello",
        "f_bytes": b"world",
        "f_float_optional": 6.1,
        "f_array_of_float": [1.0, 2.0, 3.0],
        "f_array_of_double": [10.0, 20.0, 30.0],
        "f_array_of_string": ["a", "b", "c"],
        "f_another_array_of_string": ["a", "b", "c"],
        "f_optional_array_of_float": [1.0, 2.0, 3.0],
    },
    {
        "f_null": None,
        "f_boolean": False,
        "f_int": 6,
        "f_long": -123,
        "f_float": -1.234,
        "f_another_float": 6.0,
        "f_double": 6.789,
        "f_string": "",
        "f_bytes": b"",
        "f_float_optional": None,
        "f_array_of_float": [4.0, 5.0, 6.0],
        "f_array_of_double": [40.0, 50.0, 60.0],
        "f_array_of_string": ["c", "a", "b"],
        "f_another_array_of_string": ["c", "def"],
        "f_optional_array_of_float": None,
    },
]

with open("/tmp/toy_codex-null.avro", "wb") as out:
  writer(out, parsed_schema, records, codec="null")

with open("/tmp/toy_codex-deflate.avro", "wb") as out:
  writer(out, parsed_schema, records, codec="deflate")
```

## toy_vector_sequence_from_polars.avro

A numerical vector sequence dataset created with polars.

```python
def gen_dataset(n=100, dim=2):

  def gen_points():
    num_pts = np.random.randint(0, 10)
    return np.random.uniform(size=[num_pts, dim])

  def get_label(pts):
    center = np.ones(shape=dim) / 2

    for i in range(pts.shape[0]):
      d = np.linalg.norm(pts[i, :] - center)
      if d <= 0.2:
        return "1"

    return "0"

  def gen_example():
    pts = gen_points()
    label = get_label(pts)
    return {
        "f1": pts.tolist(),
        "label": label,
    }

  ds = {"label": [], "f1": []}
  for _ in range(n):
    example = gen_example()
    ds["f1"].append(example["f1"])
    ds["label"].append(example["label"])
  return ds


pl.DataFrame(gen_dataset()).write_avro("toy_vector_sequence_from_polars.avro")
```

## toy_vector_sequence_from_fastavro.avro

A numerical vector sequence dataset created with fastavro.

```python
def export_dataset_as_avro(ds: dict[str, Any], path: str):
  schema = fastavro.parse_schema({
      "fields": [
          {
              "name": "f1",
              "type": {
                  "items": {
                      "items": "float", "type": "array"},"type": "array",
              },
          },
          {
              "name": "label",
              "type": "string",
          },
      ],
      "name": "",
      "type": "record",
  })

  records = []
  keys = list(ds.keys())
  for i in range(len(ds[keys[0]])):
    item = {}
    for key in keys:
      item[key] = ds[key][i]
    records.append(item)

  with open(path, "wb") as f:
    fastavro.writer(f, schema, records)

# Note: "gen_dataset" is defined in the "toy_vector_sequence_from_polars.avro" section.

export_dataset_as_avro(gen_dataset(),"toy_vector_sequence_from_fastavro.avro")
```

## file_deletions_train.csv and file_deletions_test.csv

Those are survival datasets that represent file lifetimes, with left-truncation
and right-censoring effects. The file lifetime is correlated to three
categorical features (user, location & class), and two numerical ones (size &
temperature).

They are generated with:

```python
def generate_random_covariance_matrix(n):
  A = numpy.random.rand(n, n)
  Q, R = numpy.linalg.qr(A)
  eigenvalues = numpy.exp(numpy.random.randn(n))
  D = numpy.diag(eigenvalues)
  cov_matrix = Q @ D @ Q.T
  return cov_matrix


def generate_correlated_samples(sample_size, distributions, covariance_matrix):
  n = len(distributions)
  covariance_matrix = numpy.array(covariance_matrix)
  if covariance_matrix.shape != (n, n):
    raise ValueError('Covariance matrix must have shape (n, n).')

  samples = []
  mvn_samples = scipy.stats.multivariate_normal.rvs(
      cov=covariance_matrix, size=sample_size
  )
  for i, distribution in enumerate(distributions):
    uniform_sample = scipy.stats.norm.cdf(mvn_samples[:, i])
    samples.append(distribution.ppf(uniform_sample))
  return samples


def coalesce_cols(df, name, count):
  popped_columns = {}
  for col in [f'{name}_{i}' for i in range(count)]:
    popped_columns[col] = df.pop(col)
  extracted_cols = pandas.DataFrame(popped_columns)
  df[name] = extracted_cols.idxmax(axis=1)

def generate_dataset(size, covariance_matrix, betas, base_hazard):
  samples = generate_correlated_samples(
      size, distributions.values(), covariance_matrix
  )
  data = {k: s for k, s in zip(distributions.keys(), samples)}
  df = pandas.DataFrame(data)

  # Derive proportional hazard lifetime.
  df['hazard'] = numpy.dot(df.values, betas)
  df['lifetime'] = 1 - numpy.log(numpy.random.uniform(size=len(df))) / numpy.exp(df['hazard']) / base_hazard

  # Add age.
  observation_window = 10
  df['age'] = scipy.stats.randint(1, 300).rvs(len(df))

  # Apply left-truncation
  left_truncated = df['age'] - df['lifetime'] > observation_window
  df = df.loc[~left_truncated]

  # Apply right-censoring, turning the age from "age at the end of
  # the observation window" to "age at which it entered the observation
  # window".
  right_censored = df['lifetime'] > df['age']
  df['event'] = ~right_censored
  df.loc[right_censored, 'lifetime'] = df['age']
  df['age'] -= observation_window
  df.loc[df['age'] < 0, 'age'] = 0

  coalesce_cols(df, 'user', 6)
  coalesce_cols(df, 'location', 4)
  coalesce_cols(df, 'class', 5)
  return df

distributions = {
    'size': scipy.stats.pareto(b=2),
    'temperature': scipy.stats.norm(1000, 200),
    'user_0': scipy.stats.randint(0, 2),
    'user_1': scipy.stats.randint(0, 2),
    'user_2': scipy.stats.randint(0, 2),
    'user_3': scipy.stats.randint(0, 2),
    'user_4': scipy.stats.randint(0, 2),
    'user_5': scipy.stats.randint(0, 2),
    'location_0': scipy.stats.randint(0, 2),
    'location_1': scipy.stats.randint(0, 2),
    'location_2': scipy.stats.randint(0, 2),
    'location_3': scipy.stats.randint(0, 2),
    'class_0': scipy.stats.randint(0, 2),
    'class_1': scipy.stats.randint(0, 2),
    'class_2': scipy.stats.randint(0, 2),
    'class_3': scipy.stats.randint(0, 2),
    'class_4': scipy.stats.randint(0, 2),
}
covariance_matrix = generate_random_covariance_matrix(len(distributions))
betas = [.01, -.002, 1, -1, 2, -2, 3, -3, 1, -1, 2, -2, 1, 1, 1, 1, 1]
base_hazard = .01

train_df = generate_dataset(100000, covariance_matrix, betas, base_hazard)
test_df = generate_dataset(10000, covariance_matrix, betas, base_hazard)

with open('file_deletions_train.csv', 'w') as train_f:
  train_df.drop('hazard', axis=1).to_csv(train_f, index=False)

with open('file_deletions_test.csv', 'w') as test_f:
  test_df.drop('hazard', axis=1).to_csv(test_f, index=False)
```
