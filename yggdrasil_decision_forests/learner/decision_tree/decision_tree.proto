/*
 * Copyright 2022 Google LLC.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     https://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

syntax = "proto2";

package yggdrasil_decision_forests.model.decision_tree.proto;

import "yggdrasil_decision_forests/utils/distribution.proto";

option java_outer_classname = "DecisionTreeLearner";

// Training configuration for the Random Forest algorithm.
message DecisionTreeTrainingConfig {
  // Next ID: 26

  // Basic parameters.

  // Maximum depth of the tree. max_depth=1 means that all trees will be roots.
  // If max_depth=-1, the depth of the tree is not limited.
  optional int32 max_depth = 1 [default = 16];

  // Minimum number of examples in a node.
  optional int32 min_examples = 2 [default = 5];

  oneof control_num_candidate_attributes {
    // Number of unique valid attributes tested for each node. An attribute is
    // valid if it has at least a valid split. If
    // "num_candidate_attributes=0" the value is set to the classical default
    // value for Random Forest: "sqrt(number of input attributes)" in case of
    // classification and "number of input attributes/3" in case of regression.
    // If "num_candidate_attributes=-1", all the attributes are tested.
    int32 num_candidate_attributes = 6 [default = 0];

    // If set, replaces "num_candidate_attributes" with the
    // "number_of_input_features x num_candidate_attributes_ratio". The possible
    // values are between ]0, and 1] as well as -1. If not set or equal to -1,
    // the "num_candidate_attributes" is used.
    float num_candidate_attributes_ratio = 17 [default = -1];
  }

  // Advanced parameters.

  // Whether to check the "min_examples" constraint in the split search (i.e.
  // splits leading to one child having less than "min_examples" examples are
  // considered invalid) or before the split search (i.e. a node can be
  // derived only if it contains more than "min_examples" examples). If false,
  // there can be nodes with less than "min_examples" training examples.
  optional bool in_split_min_examples_check = 3 [default = true];

  // Whether to store the full distribution (e.g. the distribution of all
  // the possible label values in case of classification) or only the top label
  // (e.g. the most representative class). This information is used for model
  // interpretation as well as in case of "winner_take_all_inference=false".
  // In the worst case, this information can account for a significant part of
  // the model size.
  optional bool store_detailed_label_distribution = 4 [default = true];

  // INFO: Use "pure_serving_model" instead of
  // "keep_non_leaf_label_distribution". "pure_serving_model" is more general,
  // works in more situations, and removes more unused data from the model.
  //
  // Whether to keep the node value (i.e. the distribution of the labels of the
  // training examples) of non-leaf nodes. This information is not used during
  // serving, however it can be used for model interpretation as well as hyper
  // parameter tuning. In the worst case, this can account for half of the model
  // size.
  //
  // keep_non_leaf_label_distribution=false is not compatible with monotonic
  // constraints.
  optional bool keep_non_leaf_label_distribution = 5 [default = true];

  // Method used to handle missing attribute values.
  enum MissingValuePolicy {
    // Missing attribute values are imputed, with the mean (in case of numerical
    // attribute) or the most-frequent-item (in case of categorical attribute)
    // computed on the entire dataset (i.e. the information contained in the
    // data spec).
    GLOBAL_IMPUTATION = 0;

    // Missing attribute values are imputed with the mean (numerical attribute)
    // or most-frequent-item (in the case of categorical attribute) evaluated on
    // the training examples in the current node.
    LOCAL_IMPUTATION = 1;

    // Missing attribute values are imputed from randomly sampled values from
    // the training examples in the current node. This method was proposed by
    // Clinic et al. in "Random Survival Forests"
    // (https://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908043).
    RANDOM_LOCAL_IMPUTATION = 2;
  }
  optional MissingValuePolicy missing_value_policy = 7
      [default = GLOBAL_IMPUTATION];

  // If true, the tree training evaluates conditions of the type "X is NA" i.e.
  // "X is missing".
  optional bool allow_na_conditions = 8 [default = false];

  optional GreedyForwardCategoricalSet categorical_set_greedy_forward = 12;

  // How to grow the tree.
  oneof growing_strategy {
    // [Default strategy] Each node is split independently of the other nodes.
    // In other words, as long as a node satisfy the splits constraints (e.g.
    // maximum depth, minimum number of observations), the node will be split.
    //
    // This is the "classical" way to grow decision trees.
    GrowingStrategyLocalBest growing_strategy_local = 13;

    // The node with the best loss reduction among all the nodes of the tree is
    // selected for splitting.
    //
    // This method is also called "best first" or "leaf-wise growth".
    // See "Best-first decision tree learning", Shi
    // (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.2862&rep=rep1&type=pdf)
    // and "Additive logistic regression : A statistical view of boosting",
    // Friedman et al. (https://projecteuclid.org/euclid.aos/1016218223) for
    // more details.
    GrowingStrategyGlobalBest growing_strategy_best_first_global = 14;
  }

  optional NumericalSplit numerical_split = 15;

  // Options related to the learning of categorical splits.
  optional Categorical categorical = 16;

  // Generate an error (if true) or a warning (if false) when the statistics
  // exported by splitters don't match the observed statistics.
  //
  // This fields is used in the unit tests.
  optional bool internal_error_on_wrong_splitter_statistics = 18
      [default = false];

  // What structure of split to consider.
  oneof split_axis {
    // Axis aligned splits (i.e. one condition at a time). This is the
    // "classical" way to train a tree. Default value.
    AxisAlignedSplit axis_aligned_split = 19;

    // Sparse oblique splits (i.e. splits one a small number of features) from
    // "Sparse Projection Oblique Random Forests", Tomita et al., 2020. These
    // splits are tested iif. "sparse_oblique_split" is set.
    SparseObliqueSplit sparse_oblique_split = 20;

    // Oblique splits from "Classification Based on Multivariate Contrast
    // Patterns" by  Canete-Sifuentes et al.
    MHLDObliqueSplit mhld_oblique_split = 25;
  }

  // See "split_axis".
  message AxisAlignedSplit {}

  // See "split_axis".
  message SparseObliqueSplit {
    // Controls of the number of random projections to test at each node.
    //
    // Increasing this value very likely improves the quality of the model,
    // drastically increases the training time, and doe not impact the
    // inference time.
    //
    // Oblique splits try out max(p^num_projections_exponent,
    // max_num_projections) random projections for choosing a split, where p is
    // the number of numerical features. Therefore, increasing this
    // `num_projections_exponent` and possibly `max_num_projections` may improve
    // model quality, but will also significantly increase training time.
    //
    // Note that the complexity of (classic) Random Forests is roughly
    // proportional to `num_projections_exponent=0.5`, since it considers
    // sqrt(num_features) for a split. The complexity of (classic) GBDT is
    // roughly proportional to `num_projections_exponent=1`, since it considers
    // all features for a split.
    //
    // The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
    // recommends values in [1/4, 2].
    optional float num_projections_exponent = 1 [default = 2];

    // Maximum number of projections (applied after the
    // "num_projections_exponent").
    //
    // Oblique splits try out max(p^num_projections_exponent,
    // max_num_projections) random projections for choosing a split, where p is
    // the number of numerical features. Increasing "max_num_projections"
    // increases the training time but not the inference time. In late stage
    // model development, if every bit of accuracy if important, increase this
    // value.
    //
    // The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
    // does not define this hyperparameter.
    optional int32 max_num_projections = 2 [default = 6000];

    // Minimum number of projections.
    //
    // In a dataset with very few numerical features, increasing this parameter
    // might improve model quality.
    //
    // The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
    // does not define this hyperparameter.
    optional int32 min_num_projections = 6 [default = 1];

    // Density of the projections as an exponent of the number of features.
    // Independently for each projection, each feature has a probability
    // "projection_density_factor / num_features" to be considered in the
    // projection.
    //
    // The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
    // calls this parameter `lambda` and recommends values in [1, 5].
    //
    // Increasing this value increases training and inference time (on average).
    // This value is best tuned for each dataset.
    optional float projection_density_factor = 3 [default = 2];

    // Deprecated, use `weights` instead.
    //
    // If true, the weight will be sampled in {-1,1} (default in  "Sparse
    // Projection Oblique Random Forests" (Tomita et al, 2020)). If false, the
    // weight will be sampled in [-1,1].
    optional bool binary_weight = 4 [default = true, deprecated = true];

    // Weights to apply to the projections.
    //
    // Continuous weights generally give better performance.
    oneof weights {
      BinaryWeights binary = 7;
      ContinuousWeights continuous = 8;
      PowerOfTwoWeights power_of_two = 9;
      IntegerWeights integer = 10;
    }

    // Weights sampled in {-1, 1} (default in "Sparse Projection Oblique Random
    // Forests" (Tomita et al, 2020))).
    message BinaryWeights {}

    // Weights sampled in [-1, 1]. Consistently gives better quality models than
    // binary weights.
    message ContinuousWeights {}

    // Weights sampled uniformly in the exponend space, i.e. the weights are of
    // the form $s * 2^i$ with the integer exponent $i$ sampled uniformly in
    // [min_exponent, max_exponent] and the sign $s$ sampled uniformly in {-1,
    // 1}.
    message PowerOfTwoWeights {
      optional int32 min_exponent = 1 [default = -3];
      optional int32 max_exponent = 2 [default = 3];
    }

    // Weights sampled in uniformly in the integer range [minimum, maximum].
    message IntegerWeights {
      optional int32 minimum = 1 [default = -5];
      optional int32 maximum = 2 [default = 5];
    }

    // Normalization applied on the features, before applying the sparse oblique
    // projections.
    optional Normalization normalization = 5 [default = NONE];

    enum Normalization {
      // No normalization. Logic used in the  "Sparse Projection Oblique Random
      // Forests" (Tomita et al, 2020).
      NONE = 0;

      // Normalize the feature by the estimated standard deviation on the entire
      // train dataset. Also known as Z-Score normalization.
      STANDARD_DEVIATION = 1;

      // Normalize the feature by the range (i.e. max-min) estimated on the
      // entire train dataset.
      MIN_MAX = 2;
    }

    // Maximum number of features in a projection. Set to -1 or not provided for
    // no maximum.
    //
    // Use only if a hard maximum on the number of variables is  needed,
    // otherwise prefer `projection_density_factor` for controlling the number
    // of features per projection.
    optional int32 max_num_features = 11 [default = -1];
  }

  message MHLDObliqueSplit {
    // Maximum number of attributes in the projection. Increasing this value
    // increases the training time. Decreasing this value acts as a
    // regularization. The value should be in [2, num_numerical_features]. If
    // the value is above num_numerical_features, the value is capped to
    // num_numerical_features. The value 1 is allowed but results in ordinary
    // (non-oblique) splits
    optional int32 max_num_attributes = 1 [default = 4];

    // If true, applies the attribute sampling in "num_candidate_attributes" and
    // "num_candidate_attributes_ratio". If false, all attributes are tested.
    optional bool sample_attributes = 2 [default = false];
  }

  // Uplift specific hyper-parameters.
  optional Uplift uplift = 22;

  message Uplift {
    // Minimum number of examples per treatment in a node. Only used for uplift
    // models.
    optional int32 min_examples_in_treatment = 1 [default = 5];

    // Splitter score i.e. score optimized by the splitters. Changing the
    // splitter score will impact the trained model.
    //
    // The following scores are introduced in "Decision trees for uplift
    // modeling with single and multiple treatments", Rzepakowski et al.
    //
    // Notation:
    //  p: probability of the positive outcome (categorical outcome) or average
    //    value of the outcome (numerical outcome) in the treatment group.
    //  q: probability / average value in the control group.
    enum SplitScore {
      // Score: - p log (p/q)
      // Categorical outcome only.
      KULLBACK_LEIBLER = 0;

      // Score: (p-q)^2
      // Categorical outcome only.
      // TODO: Add numerical outcome.
      EUCLIDEAN_DISTANCE = 1;

      // Score: (p-q)^2/q
      // Categorical outcome only.
      CHI_SQUARED = 2;

      // Conservative estimate (lower bound) of the euclidean distance.
      CONSERVATIVE_EUCLIDEAN_DISTANCE = 3;
    }

    optional SplitScore split_score = 2 [default = KULLBACK_LEIBLER];

    // How to order buckets having no values for one of the treatments.
    // This parameter is used exclusively for the bucket sorting during the
    // generation of some of the candidate splits. For example, for categorical
    // features with the CART splitter
    enum EmptyBucketOrdering {
      // Uses the treatment conditional mean outcome of the parent node.
      PARENT_TREATMENT_OUTCOME = 0;

      // Uses the mean outcome of the parent node.
      PARENT_OUTCOME = 1;
    }
    optional EmptyBucketOrdering empty_bucket__ordering = 3
        [default = PARENT_TREATMENT_OUTCOME];
  }

  // If set, the decision tree is trained to be honest.
  //
  // In honest trees, different training examples are used to infer the
  // structure and the leaf values. This regularization technique trades
  // examples for bias estimates. It might increase or reduce the quality of the
  // model.
  //
  // See "Generalized Random Forests", Athey et al. In this paper, Honest tree
  // are trained with the Random Forest algorithm with a sampling without
  // replacement.

  optional Honest honest = 24;

  message Honest {
    // Ratio of examples used to set the leaf values.
    optional float ratio_leaf_examples = 1 [default = 0.5];

    // If true, a new random separation is generated for each tree.
    // If false, the same separation is used for all the trees (for examples,
    // in a Gradient Boosted Trees containing multiple trees).
    optional bool fixed_separation = 2 [default = false];
  }

  message NumericalVectorSequence {
    // Number of training examples to use when evaluating the score of an anchor
    // in the anchor selection stage. Note that all the training examples are
    // used when evaluating the score of an anchor-based split.
    optional int64 max_num_test_examples = 1 [default = 1000];

    // Number of anchors generated by sampling training example observations.
    optional int32 num_random_selected_anchors = 2 [default = 100];
  }

  // Options to learn numerical vector sequence conditions.
  optional NumericalVectorSequence numerical_vector_sequence = 26;

  // Internal knobs of the algorithm that don't impact the final model.
  optional Internal internal = 21;

  message Internal {
    // How the computation of sorted values (non discretized numerical values)
    // are obtained.
    enum SortingStrategy {
      // Values are sorted within each node.
      IN_NODE = 0;

      // Values are pre-sorted into an index to speed-up training. The index
      // will be automatically ignored When using the index is slower than
      // sorting in-node or if the algorithm does not benefit from pre-sorting.
      // This method can increase significantly the amount of memory required
      // for training.
      PRESORTED = 1;

      // Always use the presorted index, even if the result would be slower.
      // For testing only.
      FORCE_PRESORTED = 2;

      // Select automatically the best method (The quickest method that does not
      // consume excessive RAM).
      AUTO = 3;
    }
    optional SortingStrategy sorting_strategy = 21 [default = AUTO];

    // If set, ensures that the effective strategy is
    // "ensure_effective_strategy". "ensure_effective_strategy" is only used in
    // unit test when the sorting strategy is not manually set i.e.
    // sorting_strategy = AUTO.
    optional SortingStrategy ensure_effective_sorting_strategy = 1;

    // If false, the score of a hessian split is:
    //   score \approx \sum_{children} sum_grad^2/sum_hessian
    //
    // If true, the score of a hessian split is:
    //   score \approx (\sum_{children} sum_grad^2/sum_hessian) -
    //     sum_grad_parent^2/sum_hessian_parent.
    //
    // This flag has two effects:
    // - The absolute value of the score is different (e.g. when looking at the
    //   variable importance).
    // - When growing the tree with global optimization, the structure of the
    //   tree might differ (however there is not impact on the structure when
    //   using the divide and conquer strategy).
    //
    // YDF used implicitly hessian_split_score_subtract_parent=false. XGBoost
    // uses hessian_split_score_subtract_parent=true but the paper is explicit
    // that this is just a possible solution. Both versions make sense (and
    // produce similar results). Another possible version would be subtracting
    // the parent gradient before the square.
    //
    // An experiment was conducted on 68 datasets, 10 folds CV, and 3 times
    // repetitions to evaluate the effect of this flags. Both methods produce
    // close models. However, in average accuracy, average auc and average
    // rank, the "false" method is better than the "true" one by a small but
    // visible margin.
    optional bool hessian_split_score_subtract_parent = 22 [default = false];

    // If true, partially checks monotonic constraints of trees after training.
    // This option is used by unit testing. That is, check that the value of a
    // positive node is greater than the value of a generative note (in case of
    // increasing monotonic constraint). If false and if a monotonic constraint
    // is not satisfied, the monotonic constraint is manually enforced.
    //
    // The current checking implementation might detect as non-monotonic trees
    // that are in fact monotonic (e.g. false positive). However, with the
    // current algorithm used to create monotonic constraints, this checking
    // algorithm cannot create false positives.
    optional bool check_monotonic_constraints = 23 [default = false];

    // If true, the splitter returns an InvalidArgumentError. This field can be
    // used to check the propagation of error to the user.
    optional bool generate_fake_error_in_splitter = 24 [default = false];
  }

  // Deprecated tag numbers.
  reserved 9, 10, 11;
}

// How to find numerical splits.
message NumericalSplit {
  enum Type {
    // Original/CART splitting. Slow but gives good (small, high accuracy)
    // models. Equivalent to XGBoost Exact.
    EXACT = 0;

    // Note: The histogram splitters are implemented as simply as possible,
    // and mainly for comparison purpose. The speed would be significantly
    // improved (especially for deep trees), but the learned  models would
    // remain the same.

    // Select candidate splits randomly between the min and max values.
    // Similar to the ExtraTrees algorithm:
    // https://link.springer.com/content/pdf/10.1007%2Fs10994-006-6226-1.pdf
    HISTOGRAM_RANDOM = 1;

    // Select the candidate splits uniformly (in the feature space) between the
    // min and max value.
    HISTOGRAM_EQUAL_WIDTH = 2;

    reserved 3;
  }

  optional Type type = 1 [default = EXACT];

  // Number of candidate thresholds. Ignored for EXACT.
  // Default:
  // HISTOGRAM_RANDOM => 1
  // HISTOGRAM_EQUAL_WIDTH => 255
  optional int32 num_candidates = 2;
}

message GreedyForwardCategoricalSet {
  // Probability for a categorical value to be a candidate for the positive set
  // in the extraction of a categorical set split. The sampling is applied
  // once per node (i.e. not at every step of the greedy optimization).
  optional float sampling = 1 [default = 0.1];

  // Maximum number of items (prior to the sampling). If more items are
  // available, the least frequent items are ignored. Changing this value is
  // similar to change the "max_vocab_count" before loading the dataset, with
  // the following exception: With "max_vocab_count", all the remaining items
  // are grouped in a special Out-of-vocabulary item. With "max_num_items", this
  // is not the case.
  optional int32 max_num_items = 2 [default = -1];

  // Minimum number of occurrences of an item to be considered.
  optional int32 min_item_frequency = 3 [default = 1];

  // Maximum number of items selected in the condition.
  // Note: max_selected_items=1 is equivalent to one-hot encoding.
  optional int32 max_selected_items = 4 [default = -1];
}

// How to handle categorical input features.
message Categorical {
  oneof algorithm {
    // CART algorithm (default).
    //
    // Find the a categorical split of the form "value \in mask". The solution
    // is exact for binary classification, regression and ranking. It is
    // approximated for multi-class classification.
    //
    // This is a good first algorithm to use. In case of overfitting (very small
    // dataset, large dictionary), the "random" algorithm is a good alternative.
    CART cart = 1;

    // One-hot encoding.
    //
    // Find the optimal categorical split of the form "attribute == param".
    // This method is similar (but more efficient) than converting converting
    // each possible categorical value into a boolean feature.
    //
    // This method is available for comparison purpose and generally performs
    // worst than other alternatives.
    OneHot one_hot = 2;

    // Best splits among a set of random candidate.
    //
    // Find the a categorical split of the form "value \in mask" using a random
    // search. This solution can be seen as an approximation of the CART
    // algorithm.
    //
    // This method is a strong alternative to CART.
    //
    // This algorithm is inspired from section "5.1 Categorical Variables" of
    // "Random Forest", 2001
    // (https://www.stat.berkeley.edu/users/breiman/randomforest2001.pdf).
    //
    Random random = 3;
  }

  // If the dictionary size of the attribute is greater or equal to
  // "arity_limit_for_random", the "random" algorithm is be used (instead of
  // the algorithm specified in "algorithm");
  optional int32 arity_limit_for_random = 4 [default = 300];

  message CART {}

  message OneHot {
    // Sampling of the item tested. 1. means that all the items will be tested.
    optional float sampling = 1 [default = 1.];
  }

  message Random {
    // Controls the number of random splits to evaluated.
    //
    // The effective number of splits is "min(max_num_trials, num_trial_offset +
    // {vocab size}^num_trial_exponent"), with "vocab size" the number of unique
    // categorical values in the node.
    optional float num_trial_exponent = 1 [default = 2];
    optional float num_trial_offset = 2 [default = 32];

    // Maximum number of candidates.
    optional int32 max_num_trials = 3 [default = 5000];
  }
}

// Specifies the local best growing strategy. No extra configuration needed.
message GrowingStrategyLocalBest {}

// Specifies the global best growing strategy.
message GrowingStrategyGlobalBest {
  // Maximum number of nodes in the tree. Set to "-1" to disable this limit.
  optional int32 max_num_nodes = 1 [default = 31];
}

// Statistics about the label values used to operate a splitter algorithm.
message LabelStatistics {
  optional int64 num_examples = 1;

  oneof type {
    Classification classification = 2;
    Regression regression = 3;
    RegressionWithHessian regression_with_hessian = 4;
  }

  message Classification {
    optional yggdrasil_decision_forests.utils.proto.IntegerDistributionDouble
        labels = 1;
  }

  message Regression {
    optional yggdrasil_decision_forests.utils.proto.NormalDistributionDouble
        labels = 1;
  }

  message RegressionWithHessian {
    optional yggdrasil_decision_forests.utils.proto.NormalDistributionDouble
        labels = 1;
    optional double sum_hessian = 2;
  }
}
