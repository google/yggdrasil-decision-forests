/*
 * Copyright 2022 Google LLC.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     https://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

syntax = "proto2";

package yggdrasil_decision_forests.model.gradient_boosted_trees.proto;

import "yggdrasil_decision_forests/model/abstract_model.proto";
import "yggdrasil_decision_forests/utils/distribution.proto";

// Header for the gradient boosted trees model.
message Header {
  // Next ID: 10

  // Number of shards used to store the nodes.
  optional int32 num_node_shards = 1;
  // Number of trees.
  optional int64 num_trees = 2;
  // Loss used to train the model.
  optional Loss loss = 3;
  // Initial predictions of the model (before any tree is applied). The semantic
  // of the prediction depends on the "loss".
  repeated float initial_predictions = 4;
  // Number of trees extracted at each gradient boosting operation.
  optional int32 num_trees_per_iter = 5 [default = 1];
  // Loss evaluated on the validation dataset. Only available is a validation
  // dataset was provided during training.
  optional float validation_loss = 6;
  // Container used to store the trees' nodes.
  optional string node_format = 7 [default = "TFE_RECORDIO"];
  // Evaluation metrics and other meta-data computed during training.
  optional TrainingLogs training_logs = 8;
  // If true, call to predict methods return logits (e.g. instead of probability
  // in the case of classification).
  optional bool output_logits = 9 [default = false];
  // Configuration options for losses.
  optional LossConfiguration loss_configuration = 10;
}

enum Loss {
  // Selects the most adapted loss according to the nature of the task and the
  // statistics of the label.
  // - Binary classification -> BINOMIAL_LOG_LIKELIHOOD.
  DEFAULT = 0;
  // Binomial log likelihood. Only valid for binary classification.
  BINOMIAL_LOG_LIKELIHOOD = 1;
  // Least square loss. Only valid for regression.
  SQUARED_ERROR = 2;
  // Multinomial log likelihood i.e. cross-entropy.
  MULTINOMIAL_LOG_LIKELIHOOD = 3;
  // DEPRECATED: Use LAMBDA_MART_NDCG.
  // LambdaMART with NDCG5
  LAMBDA_MART_NDCG5 = 4 [deprecated = true];
  // XE_NDCG_MART [arxiv.org/abs/1911.09798]
  XE_NDCG_MART = 5;
  // EXPERIMENTAl. Focal loss. Only valid for binary classification.
  // [https://arxiv.org/pdf/1708.02002.pdf]
  BINARY_FOCAL_LOSS = 6;
  // Poisson log likelihood. Only valid for regression.
  POISSON = 7;
  // Mean average error (MAE).
  MEAN_AVERAGE_ERROR = 8;
  // LambdaMART with NDCG loss. Truncation defaults to 5, configurable.
  LAMBDA_MART_NDCG = 9;
}

// Log of the training. This proto is generated during the training of the
// model and optionally exported (as a plot) in the training logs directory.
message TrainingLogs {
  // Measurements the model size and performances during the training.
  repeated Entry entries = 1;

  // Names of the metrics stored in "secondary_metrics" field. The secondary
  // metrics depends on the task (e.g. classification) and is accessible with
  // "SecondaryMetricNames()". The i-th metric name of "secondary_metric_names"
  // correspond to the i-th metric value in "training_secondary_metrics" and
  // "validation_secondary_metrics".
  repeated string secondary_metric_names = 2;

  // Number of trees in the final model. Without early stopping,
  // "number_of_trees_in_final_model" is equal to the "number_of_trees" of the
  // last "entries".
  optional int32 number_of_trees_in_final_model = 3;

  message Entry {
    // Number of trees. In the case of multi-dimensional gradients,
    // "number_of_trees" is the number of training step.
    optional int32 number_of_trees = 1;
    // Performance of the model on the training dataset.
    optional float training_loss = 2;
    repeated float training_secondary_metrics = 3;
    // Performance of the model on the validation dataset.
    optional float validation_loss = 4;
    repeated float validation_secondary_metrics = 5;
    // Average of the absolute value of the new tree predictions estimated on
    // the training dataset. See Dart paper
    // (http://proceedings.mlr.press/v38/korlakaivinayak15.pdf) for details on
    // how to interpret it.
    optional double mean_abs_prediction = 6;
    // Sub-sampling factor applied during training on top of the "sampling"
    // hyper-parameter. Currently, the "subsample_factor" is only controlled by
    // the "adapt_subsample_for_maximum_training_duration" field i.e. the
    // "subsample_factor" factor (default to 1) is reduced so the training
    // finishes in "maximum_training_duration".
    optional float subsample_factor = 7 [default = 1.];
    // Confusion between the label and the predictions.
    optional utils.proto.IntegersConfusionMatrixDouble
        validation_confusion_matrix = 8;
  }
}

message GradientBoostedTreesSerializedModel {
  optional Header header = 1;
}

extend model.proto.SerializedModel {
  optional GradientBoostedTreesSerializedModel
      gradient_boosted_trees_serialized_model = 1001;
}

message LossConfiguration {
  message LambdaMartNdcg {
    // If false, the gradient is computed using NDCG i.e. normalized-DCG. If
    // false, the gradient is computed using DCG.
    optional bool gradient_use_non_normalized_dcg = 1 [default = false];

    // Number of candidates considered when computing the NDCG loss.
    //
    // NDCG losses are usually truncated at a particular rank level (generally
    // between 4 and 10), i.e. only the highly ranked documents are considered
    // when computing the rank. A smaller values results in a model with
    // increased emphasis on the first results of the ranking.
    //
    // Note that the NDCG truncation of the cross-entropy NDCG loss must be
    // configured separately.
    optional int32 ndcg_truncation = 2 [default = 5];
  }

  message XeNdcg {
    enum Gamma {
      // For the time being, defaults to UNIFORM.
      AUTO = 0;
      // Gammas are sampled from a uniform distribution on [0, 1].
      UNIFORM = 1;
      // Gammas are set to 1 across the board. This is more appropriate for
      // click datasets with a large number of documents per query.
      ONE = 2;
    }
    optional Gamma gamma = 1 [default = UNIFORM];

    // Number of candidates considered when computing the NDCG loss.
    //
    // NDCG losses are usually truncated at a particular rank level (generally
    // between 4 and 10), i.e. only the highly ranked documents are considered
    // when computing the rank. A smaller values results in a model with
    // increased emphasis on the first results of the ranking.
    //
    // Note that the NDCG truncation of the classic NDCG loss must be configured
    // separately.
    optional int32 ndcg_truncation = 2 [default = 5];
  }

  message BinaryFocalLossOptions {
    // Exponent of the misprediction multiplier in focal loss.
    // Corresponds to the gamma parameter in
    // https://arxiv.org/pdf/1708.02002.pdf
    optional float misprediction_exponent = 1 [default = 2.0];

    // A hypertuning coefficient to multiply the loss and its gradient(s) in
    // case of a positive sample.
    // Loss and gradient on positive samples will be multiplied by
    // positive_sample_coefficient, on negative samples will be multiplied
    // by (1 - positive_sample_coefficient)
    // Corresponds to the 'alpha' parameter in
    // https://arxiv.org/pdf/1708.02002.pdf
    optional float positive_sample_coefficient = 2 [default = 0.5];
  }
  oneof loss_options {
    LambdaMartNdcg lambda_mart_ndcg = 1;
    XeNdcg xe_ndcg = 2;
    BinaryFocalLossOptions binary_focal_loss_options = 3;
  }
}
