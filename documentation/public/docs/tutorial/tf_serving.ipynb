{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtzP527ne07T"
      },
      "source": [
        "# With TF Serving\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/yggdrasil-decision-forests/blob/main/documentation/public/docs/tutorial/tf_serving.ipynb)\n",
        "\n",
        "This tutorial demonstrates how to train a YDF model, export it to the TensorFlow SavedModel format, and serve it for online and batch predictions using Google Cloud's Vertex AI. It also covers how to run the model locally with the TensorFlow Serving docker image for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1osQ1Yb2Fhw"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, let's install the necessary libraries. We need `ydf` for training and `tensorflow_decision_forests` for the export functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcxE99vze07U"
      },
      "outputs": [],
      "source": [
        "pip install ydf tensorflow_decision_forests -U -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0XkgV4We07V"
      },
      "source": [
        "## About this tutorial\n",
        "\n",
        "This tutorial shows how to train a YDF model, export it to the TensorFlow SavedModel format, and run this model in Vertex AI.\n",
        "Additionally, the tutorial shows how to manually run the TensorFlow Serving binary to make inferences with the model.\n",
        "\n",
        "## What is TF Serving?\n",
        "\n",
        "[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving) is a production environment for running machine learning models. TensorFlow Serving can run YDF models.\n",
        "\n",
        "## What is Vertex AI?\n",
        "\n",
        "[Vertex AI](https://cloud.google.com/vertex-ai/docs) is a Google Cloud solution to manage and serve ML models. Vertex AI relies on [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving) to run TensorFlow models (stored in the [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model) format). YDF models can be exported to TensorFlow SavedModel and run on TensorFlow Serving and Vertex AI with the [model.to_tensorflow_saved_model](../../py_api/GradientBoostedTreesModel/#ydf.GradientBoostedTreesModel.to_tensorflow_saved_model) method.\n",
        "\n",
        "## Important remark about TensorFlow Saved Model inputs\n",
        "\n",
        "TensorFlow Saved Model can be seen as a generic function that takes data as input and produces predictions as output. TensorFlow Serving and Vertex AI define three input formats for feeding input features and three output formats for retrieving predictions. Using the incorrect format can results in cryptic error messages. Understanding these formats is not necessary for using TensorFlow Serving and Vertex AI, but it can be helpful for debugging your pipeline. This section provides an overview of the formats.\n",
        "\n",
        "YDF allows you to select the type of format of your model using the `servo_api: bool` and `feed_example_proto: bool` argument of the `to_tensorflow_saved_model` function.\n",
        "\n",
        "**Input format #1: input instances**\n",
        "\n",
        "In this format, the data is grouped by examples, where each example is a dictionary of features in a list. The format is straightforward but not very efficient. This format is easily usable with all APIs (REST, Python, C++). **This format is used by Vertex AI for Online predictions and by Vertex AI Batch predictions on jsonl**.\n",
        "\n",
        "Here is an a list of examples, each having 3 features \"f1\", \"f2\", and \"f3\":\n",
        "\n",
        "```json\n",
        "[ {\"f1\": 1, \"f2\": 5.9, \"f3\": \"red\" }, {\"f1\": 3, \"f2\": 2.1, \"f3\": \"blue\" } ]\n",
        "```\n",
        "\n",
        "This is the default input format of the `to_tensorflow_saved_model` function i.e. `feed_example_proto=False`.\n",
        "\n",
        "**Input format #2: input feature**\n",
        "\n",
        "In this format, the data is grouped by features, where each feature is a list of values in a dictionary. The format is relatively straightforward and the most efficient. This format is easily usable with all APIs (REST, Python, C++). When possible, this is the format to use.\n",
        "\n",
        "Here is the same example in this format:\n",
        "\n",
        "```json\n",
        "{\"f1\": [1, 3], \"f2\": [5.9, 2.1], \"f3\": [\"red\", \"blue\"] }\n",
        "```\n",
        "\n",
        "This is also the default input format of the `to_tensorflow_saved_model` function i.e. `feed_example_proto=False`.\n",
        "\n",
        "**Input format #3: serialized tensorflow examples**\n",
        "\n",
        "In this last format, the data is encoded as a [Google Protobuf](https://github.com/protocolbuffers/protobuf) of [TensorFlow Example protos](https://www.tensorflow.org/api_docs/python/tf/train/Example), which is also the format used to train large TensorFlow pipelines.\n",
        "This format is not efficient for serving and relatively complex to use. When possible, try to avoid using it for inference. **This format is required by Vertex AI Batch predictions on TensorFlow Records files**.\n",
        "\n",
        "This format is enabled with `feed_example_proto=True` in the `to_tensorflow_saved_model` function.\n",
        "\n",
        "**Output format #1: predict**\n",
        "\n",
        "This format is the simplest and most efficient one. The predicted value is directly outputted by the model. The meaning of the predictions is determined by the model. A classification model, for instance, will output probabilities, whereas a regression model will output values. This is the default output format of the `to_tensorflow_saved_model` function i.e. `servo_api=False`.\n",
        "\n",
        "Here is an example of prediction for a binary classification model:\n",
        "\n",
        "```json\n",
        "{ \"prediction\": [0.2, 0.7] }\n",
        "```\n",
        "\n",
        "Here is an example of prediction for a multi-class classification model:\n",
        "\n",
        "```json\n",
        "{ \"prediction\": [[0.2, 0.1, 0.7],\n",
        "                  [0.8, 0.1, 0.1]] }\n",
        "```\n",
        "\n",
        "This format is available for Vertex AI Online predictions.\n",
        "\n",
        "**Output format #2 \u0026 3: classify and regress**\n",
        "\n",
        "In those formats, the model outputs a dictionary of values. The values depend on the model type. For instance, a classification model will output a \"score\" and a \"labels\" value. This output format is enabled with `servo_api=True` in the `to_tensorflow_saved_model` function.\n",
        "\n",
        "**This format is available for Vertex AI Online predictions, and required for Vertex AI Batch predictions.**\n",
        "\n",
        "Here is an example of prediction for a binary-classification model:\n",
        "\n",
        "```json\n",
        "{ \"scores\":  [[0.2, 0.8],\n",
        "             [0.1, 0.9]],\n",
        "  \"classes\": [[\"v1\", \"v2\"],\n",
        "             [\"v1\", \"v2\"]]}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWvg0vGqe07V"
      },
      "source": [
        "## Train a model\n",
        "\n",
        "We train a binary classification YDF model similarly to the [classification tutorial](../classification/).\n",
        "\n",
        "We load a dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKY9pzTde07W"
      },
      "outputs": [],
      "source": [
        "# Load libraries\n",
        "import ydf  # Yggdrasil Decision Forests\n",
        "import pandas as pd  # We use Pandas to load small datasets\n",
        "\n",
        "# Download a classification dataset and load it as a Pandas DataFrame.\n",
        "ds_path = \"https://raw.githubusercontent.com/google/yggdrasil-decision-forests/main/yggdrasil_decision_forests/test_data/dataset\"\n",
        "train_ds = pd.read_csv(f\"{ds_path}/adult_train.csv\")\n",
        "test_ds = pd.read_csv(f\"{ds_path}/adult_test.csv\")\n",
        "\n",
        "# Print the first 5 training examples\n",
        "train_ds.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcD7rAre07W"
      },
      "source": [
        "We train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOAx6mwse07X"
      },
      "outputs": [],
      "source": [
        "model = ydf.GradientBoostedTreesLearner(label=\"income\").train(train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PDU13MSe07Y"
      },
      "source": [
        "**Note:** While not demonstrated here, it is recommended to look at and evaluate a model before putting it into use. Use the `model.describe()` method to examine the model's structure and characteristics, and use the `model.evaluate(...)` method to assess its performance and accuracy. See the [Getting started tutorial](../getting_started) for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAIuH4_Be07Z"
      },
      "source": [
        "## Export model to TF Saved Model format\n",
        "\n",
        "TensorFlow Serving can only read models in the TensorFlow SavedModel format. Therefore, we export the YDF model to the TensorFlow SavedModel format.\n",
        "\n",
        "This step requires the [TensorFlow Decision Forests](https://www.tensorflow.org/decision_forests) library to be installed.\n",
        "\n",
        "**Info:** TensorFlow Decision Forests is a Keras 2 wrapper built on top of YDF and developed by the YDF team. For most use cases, using YDF directly is preferable as it is faster, easier to use, and compatible both with Keras 2 and Keras 3. Learn [more](../../faq/#python-ydf-and-tf-df)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jnTaU6Oe07a"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_decision_forests -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5KTvbN1e07a"
      },
      "outputs": [],
      "source": [
        "# Export the model to a TensorFlow Saved Model.\n",
        "\n",
        "# For Vertex AI Online inference.\n",
        "# The model consumes raw features values, and output raw model predictions.\n",
        "model.to_tensorflow_saved_model(\"/tmp/ydf/tf_model\", mode=\"tf\")\n",
        "\n",
        "# For Vertex AI Batch inference.\n",
        "# The model consumes TensorFlow Example protos, and returns a dictionary.\n",
        "# model.to_tensorflow_saved_model(\"/tmp/ydf/tf_model\", mode=\"tf\", servo_api=True, feed_example_proto=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc3LO8WAe07a"
      },
      "source": [
        "## Import the model in Vertex AI\n",
        "\n",
        "To import the model in Vertex AI, follow those steps:\n",
        "\n",
        "**1. Import the TensorFlow Saved Model into a Google Cloud Bucket**\n",
        "\n",
        "1. Open the [Cloud Storage page](https://pantheon.corp.google.com/storage).\n",
        "1. Create a new bucket or select an existing one.\n",
        "1. Click on \"Upload folder\" and select the model exported previously, which is `/tmp/ydf/tf_model` in this example.\n",
        "\n",
        "The model bucket should contain a file called `saved_model.pb`. For example, if you upload the model to the `gs://my_bucket` bucket, the file `gs://ydf_model_2/tf_model/saved_model.pb` should be present.\n",
        "\n",
        "**2. Register model in Vertex AI**\n",
        "\n",
        "1. Open to the Vertex AI Model Registry page.\n",
        "1. Click \"Import\" and select the model from the cloud bucket.\n",
        "1. In the \"Import Model\" dialog, configure the following options:\n",
        "    - **Name:** Enter a name for the model.\n",
        "    - **Model framework:** Select TensorFlow.\n",
        "    - **Model framework version:** Select the most recent version, which is 2.13 at the time of this writing.\n",
        "    - **Accelerator:** Select None.\n",
        "    - **Model artifact location:** Specify the Google Cloud Storage (GCS) path to the model artifacts, e.g., `gs://my_bucket/tf_model/`.\n",
        "    - **Use optimized TensorFlow runtime:** Disable this field. Decision Forests do not work with this neural network specific optimization.\n",
        "1. Leave the other options with their default values and click \"Continue.\"\n",
        "1. When prompted about explainability, select \"No explainability\" and click \"Import.\"\n",
        "\n",
        "The model will be imported in a few minutes. You can monitor the progress at the top-right corner of the Model Registry page. Once imported, the model will appear in the list of registered models.\n",
        "\n",
        "The model is now ready for inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_XmCYhle07b"
      },
      "source": [
        "### Online predictions\n",
        "\n",
        "The model can be deployed to an \"endpoint\" and queried remotely via the Cloud REST API.\n",
        "\n",
        "1. Open the [Cloud Model Registry page](https://pantheon.corp.google.com/vertex-ai/models).\n",
        "1. Select the model, open the tab \"Deploy and test\" and click on \"Deploy to endpoint\".\n",
        "1. Configure the end point as follow:\n",
        "    - **Endpoint name:** Enter a name for the endpoint.\n",
        "    - **Machine type:** Select the smallest possible machine e.g. `n1-standard-2`\n",
        "1. Click on \"Deploy\"\n",
        "\n",
        "The endpoint is now being deployed.\n",
        "\n",
        "In the \"Test your model\" section, query the model with the following JSON request:\n",
        "\n",
        "```json\n",
        "{\n",
        "   \"instances\":[\n",
        "      {\n",
        "         \"age\":39,\n",
        "         \"workclass\":\"State-gov\",\n",
        "         \"fnlwgt\":77516,\n",
        "         \"education\":\"Bachelors\",\n",
        "         \"education_num\":13,\n",
        "         \"marital_status\":\"Never-married\",\n",
        "         \"occupation\":\"Adm-clerical\",\n",
        "         \"relationship\":\"Not-in-family\",\n",
        "         \"race\":\"White\",\n",
        "         \"sex\":\"Male\",\n",
        "         \"capital_gain\":2174,\n",
        "         \"capital_loss\":0,\n",
        "         \"hours_per_week\":40,\n",
        "         \"native_country\":\"United-States\"\n",
        "      }\n",
        "   ]\n",
        "}\n",
        "```\n",
        "\n",
        "The result will be:\n",
        "\n",
        "```json\n",
        "{\n",
        " \"predictions\": [\n",
        "   0.0186043456\n",
        " ],\n",
        " \"deployedModelId\": \"2255069761266253824\",\n",
        " \"model\": \"projects/734980258708/locations/us-central1/models/8572427068350922752\",\n",
        " \"modelDisplayName\": \"tf_model_servoF_protoF\",\n",
        " \"modelVersionId\": \"1\"\n",
        "}\n",
        "```\n",
        "\n",
        "This predictions indicates that the model that the positive class has 1.66% chance to be true. In other words, the model predictions the negative class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqNRyx0Ae07e"
      },
      "outputs": [],
      "source": [
        "print(\"The positive and negative classes are:\", model.label_classes())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXHfw10Ce07e"
      },
      "source": [
        "### Batch predictions\n",
        "\n",
        "Now, let's perform batch predictions with the model. This involves uploading a file containing instances, generating predictions, and retrieving the results in a JSON file.\n",
        "\n",
        "We loaded the training dataset from [a CSV file](https://github.com/google/yggdrasil-decision-forests/blob/main/yggdrasil_decision_forests/test_data/dataset/adult_train.csv). However, using CSV files with TensorFlow SavedModel can lead to errors. To avoid issues, we'll use TensorFlow's official format for datasets of examples, a [TFRecord file of tf.train.Example protobufs](https://www.tensorflow.org/tutorials/load_data/tfrecord). Fortunately, the test dataset is readily available in this format [here](https://github.com/google/yggdrasil-decision-forests/blob/main/yggdrasil_decision_forests/test_data/dataset/adult_test.recordio.gz).\n",
        "\n",
        "1. Download the \"adult_test.recordio.gz\" on your computer.\n",
        "1. Open the [Cloud Storage page](https://pantheon.corp.google.com/storage).\n",
        "1. In the bucket you already created, click on \"Upload file\" and select the file `adult_test.recordio.gz`.\n",
        "1. In the Cloud storage page, select the file and rename it `adult_test.tfrecord.gz`.\n",
        "    -  Vertex AI detect file format by extension.\n",
        "1. Open the [Cloud Model Registry page](https://pantheon.corp.google.com/vertex-ai/models).\n",
        "1. Select the model, open the tab \"Batch predict\" and click on \"Create batch prediction\".\n",
        "1. Configure the batch prediction as follow:\n",
        "    - **Batch prediction name:** Enter a name for the predictions.\n",
        "    - **File on Cloud Storage (JSONL, CSV, TFRecord, and TFRecord (GZIP)):** Select the `adult_test.tfrecord.gz` file.\n",
        "    - **Destination path:** Select the bucket containing the model and the dataset.\n",
        "    - **Number of compute nodes:** Enter \"1\". This is a small dataset that does not require much power.\n",
        "    - **Machine type:** Select the smallest possible machine e.g. `n1-standard-2`\n",
        "1. Click on \"Create\"\n",
        "\n",
        "The predictions are currently being calculated. When the computation is complete, you will find them in a newly created JSON file located in your bucket.\n",
        "\n",
        "Here are the first five lines of the generated file:\n",
        "\n",
        "```json\n",
        "{\"prediction\": {\"scores\": [0.981395662, 0.0186043456], \"classes\": [\"\u003c=50K\", \"\u003e50K\"]}}\n",
        "{\"prediction\": {\"scores\": [0.638690472, 0.361309558], \"classes\": [\"\u003c=50K\", \"\u003e50K\"]}}\n",
        "{\"prediction\": {\"scores\": [0.161411345, 0.838588655], \"classes\": [\"\u003c=50K\", \"\u003e50K\"]}}\n",
        "{\"prediction\": {\"scores\": [0.956144333, 0.0438556746], \"classes\": [\"\u003c=50K\", \"\u003e50K\"]}}\n",
        "{\"prediction\": {\"scores\": [0.970823526, 0.0291764941], \"classes\": [\"\u003c=50K\", \"\u003e50K\"]}}\n",
        "```\n",
        "\n",
        "You can see the predicted probability for each example and each class. For instance, on the first example, the model predicts class \"\u003c=50K\" with a probability of 98.14%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfYBrqDte07f"
      },
      "source": [
        "## Run the model in local TensorFlow Serving [Advanced]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIxEdMbve07v"
      },
      "source": [
        "It is possible to start the TensorFlow Serving binary locally or on a remote machine, and send request with the TensorFlow Serving REST API. Let's show how it is done:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UoSl3c4e07v"
      },
      "source": [
        "To test our model, we start a local version of TF Serving following the [tf serving setup instructions](https://github.com/tensorflow/serving#set-up).\n",
        "\n",
        "In a separate terminal, type:\n",
        "\n",
        "```shell\n",
        "cd /tmp/ydf\n",
        "\n",
        "docker run -t --rm -p 8501:8501 \\\n",
        "    -v /tmp/tf/tf_model:/models/my_saved_model/1 \\\n",
        "    -e MODEL_NAME=my_saved_model \\\n",
        "    tensorflow/serving\n",
        "```\n",
        "\n",
        "**Note:** TensorFlow Serving expects the model path to follow the structure: `models/\u003cMODEL_NAME\u003e/\u003cVERSION\u003e`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXHRxzU8e07w"
      },
      "source": [
        "Once the TensorFlow Serving server is up and running, you can send prediction requests.\n",
        "Here is an example of each:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPZgme0_e07w"
      },
      "source": [
        "**Predictions with the input instances format:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGlBTPDQe07w",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "!curl http://localhost:8501/v1/models/my_saved_model:predict -X POST \\\n",
        "    -d '{\"instances\": [{\"age\":39,\"workclass\":\"State-gov\",\"fnlwgt\":77516,\"education\":\"Bachelors\",\"education_num\":13,\"marital_status\":\"Never-married\",\"occupation\":\"Adm-clerical\",\"relationship\":\"Not-in-family\",\"race\":\"White\",\"sex\":\"Male\",\"capital_gain\":2174,\"capital_loss\":0,\"hours_per_week\":40,\"native_country\":\"United-States\"}]}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5MmXtbde07x"
      },
      "source": [
        "**Predictions with the input features format:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N65FA8s5e07x",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "!curl http://localhost:8501/v1/models/my_saved_model:predict -X POST \\\n",
        "    -d '{\"inputs\": {\"age\":[39],\"workclass\":[\"State-gov\"],\"fnlwgt\":[77516],\"education\":[\"Bachelors\"],\"education_num\":[13],\"marital_status\":[\"Never-married\"],\"occupation\":[\"Adm-clerical\"],\"relationship\":[\"Not-in-family\"],\"race\":[\"White\"],\"sex\":[\"Male\"],\"capital_gain\":[2174],\"capital_loss\":[0],\"hours_per_week\":[40],\"native_country\":[\"United-States\"]} }'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
